Official Google Cloud Certified Associate Cloud Engineer Study Guide
====================================================================

Assessment Test
---------------

- Machine type, boot disk image or container image, zone, and labels are all configuration parameters or attributes of a VM
- **gsutil mb** is the specific command for creating, or making, a bucket
- Create a lifecycle management configuration policy specifying an age of 90 days and SetStorageClass as nearline is the most efficient way to meet object management policy requirement. Read more: Storage classes https://cloud.google.com/storage/docs/storage-classes, Object Lifecycle Management https://cloud.google.com/storage/docs/lifecycle
- **gsutil rsync** to synchronize the contents of the two buckets
- VPCs are **Global** resources. Google operates a global network, and VPCs are resources that can span that global network
- **gcloud** by default will retry a failed network operation and will wait a long time before each retry. The time to wait is calculated using a truncated binary exponential back-off strategy
- Only Google Spanner and Cloud SQL databases support transactions and have a SQL interface. Datastore has transactions but does not support fully compliant SQL; it has a SQL-like query language. Cloud Storage does not support transactions or SQL
- App Engine is a PaaS that allows developers to deploy full applications without having to manage servers or clusters. Compute Engine and Kubernetes Engine require management of servers. Cloud Functions is suitable for short-running but not full applications
- BigQuery is designed for petabyte-scale analytics and provides a SQL interface
- Cloud Dataflow allows for stream and batch processing of data and is well suited for this kind of ETL work. Dataproc is a managed Hadoop and Spark service that is used for big data analytics
- Preemptible virtual machines may be shut down at any time but will always be shut down after running 24 hours by Google
- **Organizations, folders, and projects** are the components used to manage an organizational hierarchy. **Buckets, directories, and subdirectories** are used to organize storage
- Cloud Dataproc is the managed Spark service. Cloud Dataflow is for stream and batch processing of data, BigQuery is for analytics
- To create a custom role, a user must possess which role **iam.roles.create**

**Google Cloud Function**

image::Associate Cloud Engineer Study Guide - Cloud Function.png[Google Cloud Function]


Chapter 1 - Overview of Google Cloud Platform
---------------------------------------------

- Object storage, like Cloud Storage, provides redundantly stored objects without limits on the amount of data you can store
- Block sizes in a block storage system can vary. Block size is established when a file system is created
- Firewalls are software-defined network controls that limit the flow of traffic into and out of a network or subnetwork. Routers are used to move traffic to appropriate destinations on the network. Identity access management is used for authenticating and authorizing users
- Specialized services in GCP are serverless
- Investing in servers should be based on demand for server capacity
- The characteristics of the server, such as the number of virtual servers, the amount of memory, and the region where you run the VM, influence the cost
- Containers give the most flexibility for using the resources of a cluster efficiently and orchestration platforms reduce the operations overhead
- Cloud Filestore is based on Network Filesystem (NSF), which is a distributed file management system
- When you create a network, it is treated as a virtual private cloud. Resources are added to the VPC and are not accessible outside the VPC unless
you explicitly configure them to be
- Caches use memory, and that makes them the fastest storage type for reading data. Caches are data stores on the backend of distributed systems, not the clients. Caches can get out of sync with the system of truth because the system of truth could be updated, but the cache may not be updated
- Cloud providers have large capacity and can quickly allocate those resources to different customers. With a mix of customers and workloads, they can optimize the allocation of resources
- Specialized services are monitored by Google so users do not have to monitor them. Specialized services provide a specific compute functionality but do not require the user to configure any resources. They also provide APIs
- Attached drives are block storage devices. Cloud Storage is the object storage service and does not attach directly to a VM
- Databases require persistent storage on block devices. Object storage does not provide data block or file system storage


Chapter 2 - Google Cloud Computing Services
-------------------------------------------

GCP services list:

- Computing resources - Compute Engine, Kubernetes / Containers Engine, App Engine (standard / flexible environment), Cloud Functions (event-driven processing, short-running code)
- Storage resources - Cloud Storage (for object storage, single unit of data and multiple regions), Persistent Disk (block storage), Cloud Storage for Firebase, Cloud Filestore (shared file system, NFS easily to mount)
- Databases - Cloud SQL (managed relational database without having to attend to database administration tasks, such as backing up databases or patching database software), Cloud Spanner (globally distributed relational database, with the ability to scale horizontally, supports ANSI 2011 standard SQL), Cloud Bigtable (NoSQL as wide-column data model, low-latency write and read operations, support millions of operations per second, Hbase API), Cloud Datastore (NoSQL document database, collection of key-value pair, flexible schemas, REST API, shard or partition, supports transactions, indexes, and SQL-like queries), Cloud Memorystore (in-memory cache service, managed Redis service), Cloud Firestore (NoSQL database service designed as a backend for highly scalable web and mobile applications, includes a Datastore mode, which enables applications written for Datastore to work with Cloud Firebase)
- Networking services - Virtual Private Cloud (can span the globe without relying on the public Internet), Cloud Load Balancing (distribute the workload within and across regions, adapt to failed or degraded servers, and autoscale your compute resources to accommodate changes in workload), Cloud Armor, Cloud CDN, Cloud Interconnect (interconnects and peering), Cloud DNS (automatically scale)
- Identity management and security (users, roles, and privileges, groups of related permissions can be bundled into roles)
- Development tools - Cloud SDK
- Management tools - Stackdriver, Monitoring, Logging, Error Reporting, Trace, Debugger, Profiler
- Specialized services - Apigee API Platform, Data Anylytics (BigQuery, Cloud Dataflow, Dataproc, Dataprep), AI and Machine Learning (Cloud AutoML, Machine Learning Engine, NLP, Vision)

- Container is another approach to isolating computing resources is to use features of the host operating system to isolate processes and resources without hypervisor. No guest operating systems run on top of the container manager. Containers make use of host operating system functionality, while the operating system and container manager ensure isolation between the running containers
- App Engine is well suited for web and mobile backend applications
- A zone is considered a single failure domain
- Load balancers can route workload based on network-level or application-level rules. GCP load balancers can distribute workloads globally
- Why Çloud, enable customers to focus on application development while the cloud provider takes on more responsibility for maintaining the underlying compute infrastructure
- App Engine flexible environments allow you to run containers on the App Engine PaaS
- Cloud CDN acts as a first line of defense in the case of DDoS attacks
- Stackdriver Logging is used to consolidate and manage logs generated by applications and servers
- Cloud SQL does not have global transaction
- Dataproc is designed to execute workflows in both batch and streaming modes
- Error reporting consolidates crash information


Chapter 3 - Projects, Service, Accounts, and Billing
----------------------------------------------------

- All resources are grouped, organized and managed within **resource hierarchy** (Organisation, Folder, Project). Organization policies are defined in terms of constraints on resources in the **resource hierarchy**
- **IAM** lets you assign permissions so users or roles can perform specific operations in the cloud. The **Organization Policy Service** lets you specify limits on the ways resources can be used. **IAM** specifies who can do things, and the **Organization Policy Service** specifies what can be done with resources
- Organisation Administrator Identity, Access Management IAM roles to manage the organisation
- The users with the Organization Administrator IAM role are responsible for the following:
** Defining the structure of the resource hierarchy
** Defining identity access management policies over the resource hierarchy
** Delegating other management roles to other users
- Project Creator (with **resourcemanager.projects.create** permission) and Billing Account Creator IAM roles to all users in the domain
- Projects must have billing accounts associated with them. A billing account can be associated with more than one project
- It is in projects that we create resources, use GCP services, manage permissions, and manage billing options
- Organization will have a quota of projects it can create. Google makes decisions about project quotas based on typical use, the customer’s usage history, and other factors
- List constraints:
** Allow a specific set of values
** Deny a specific set of values
** Deny a value and all its child values
** Allow all allowed values
** Deny all values
- Boolean Constraints: **constraints/compute.disableSerialPortAccess**
- To Policy Evaluation, policies are inherited and cannot be disabled or overridden by objects lower in the hierarchy
- Inherited policies can be ONLY overridden by defining a policy at a folder or project level. Service accounts and billing accounts are not part of the resource hierarchy and are not involved in overriding policies
- Role is a collection of permissions
** **Primitive roles** are building blocks for other roles, including Owner, Editor, Viewer. Primitive roles grant wide ranges of permissions that may not always be needed by a user. It is a best practice to use Predefined roles instead of Primitive roles when possible
** **Predefined roles** provide granular access to resources, designed for GCP products and services
** **Custom roles** allow cloud administrators to create and administer their own roles. Not all permissions are available in **Custom roles**
- Service accounts are resources managed by administrators. Resources can perform operations that the Service account has permission to perform.
- Service accounts are identities assigned to roles
- Two types of Service accounts:
** User managed Service accounts
** Google managed Service accounts
- Service accounts can be managed as a group of accounts at the **project level** or at the **individual service account level**. When a user is granted **iam.serviceAccountUser** at the project level, that user can manage all Service accounts in the project. If a new Service account is created, they will automatically have privilege to manage that Service account
- When a Service account is created, Google generates encrypted keys for authentication
- Service accounts are resources that are managed by administrators
- Users with the Organization IAM role are not necessarily responsible for determining what privileges should be assigned to users. That is determined based on the person’s role in the organization and the security policies established within the organization
- Billing accounts: self-serve (paid by credit card or direct debit from a bank account, costs are charged automatically) and invoiced
- A budget is associated with a billing account, not a project
- A self-service Billing account is appropriate only for amounts that are within the credit limits of credit cards
- Billing data can be exported to either a BigQuery database or a Cloud Storage file
- Stackdriver is a set of services for monitoring, logging, tracing, and debugging applications and resources. For monitoring and logging data to be saved into Stackdriver, need to create a workspace to save it
- Strakdriver workspaces are linked to projects, not individual resources


Chapter 4 - Introduction to Computing in Google Cloud
-----------------------------------------------------

- App Engine (dynamic and resident instances). The App Engine standard environment can autoscale down to no instances when there is no load and thereby minimize costs. App Engine flexible environment is similar to the Kubernetes Engine, and flexible environment will always be **at least one container** running with your service
- High performance computing clusters can use preemptible machines because work on a preemptible machine can be automatically rescheduled for another node on the cluster when a server is preempted
- Kubernetes administrates clusters of virtual and bare-metal machines, and is designed to support clusters that run a variety of applications.
- A group containers in Kubernetes called pods. Containers within a single pod share storage, network resources, an IP address and port space. A pod is a logically single unit for providing a service. A group of running identical pods is called a deployment. The identical pods are referred to as replicas.
- Kubernetes Engine is for large-scale applications that require high availability and high reliability. Kubernetes manage services which have different lifecycles and scalability requirements as a logical unit and at levels of abstraction
- Kubernetes uses 25 percent of memory up to 4GB and then slightly less for the next 4GB, and it continues to reduce the percentage of additional memory down to 2 percent of memory over 128GB; takes 6 percent CPU resources of the first core, down to 0.25 percent of any cores above four cores
- Kubernetes does not provide vulnerability scanning. GCP does have a Cloud Security Scanner product, but that is designed to work with App Engine to identify common application
vulnerabilities
- Cloud Functions provides the “glue” between services
- All Google regions have the same level of service level agreement, so reliability is the same
- Preemptible VM can save a snapshot and use that to create a new regular instance
- Custom machine types can have between 1 and 64 vCPUs and up to 6.5GB of memory per vCPU


Chapter 5 - Computing with Compute Engine Virtual Machines
----------------------------------------------------------

- All operations you perform will apply to resources in the selected project
- The first time you try to work a VM you will have to create a billing account. When you start using the console, create a project, only if billing is enabled
- A zone is a data center–like facility within a region. Different zones may have different machine types available, so you will need to specify a region first and then a zone to determine the set of machine types available
- The boot disk type, which can be either Standard Persistent Disk or SSD Persistent Disk
- Labels and a general description will help track numbers of VMs and their related costs. --labels parameter and specify the key followed by an equal sign followed by the value, e.g., KEYS=VALUE
- Metadata can specify key-value pairs associated with the instance. These values are stored in a metadata server, which is available for querying using the Compute Engine API. Metadata tags are especially useful if you have a common script you want to run on startup or shutdown but want the behavior of the script to vary according to some metadata values
- Availability Policy: Preemptibility, Automatic restart, On host maintenance
- Shielded VM is an advanced set of security controls that includes Integrity Monitoring, a check to ensure boot images have not been tampered with, including Secure Boot, Virtual Trusted Platform Module, Integrity Monitoring
- Sole Tenancy is used if you need to run your VMs on physical servers that only run your VMs
- The two operations are using the book disk configuration are adding a new disk and attaching an existing disk. Reformatting an existing disk is not an option
- If you can tolerate unplanned disruptions, use preemptible VMs
- **gcloud** commands start with gcloud followed by a service, such as compute, followed by a resource type, such as instances, followed by a command or verb


Chapter 6 - Managing Virtual Machines
-------------------------------------

- The Reset in VM Connect drop down menu is to restarts a VM
- VM instance can filter by: Labels, Internal IP, External IP, Status, Zone, Network, Deletion protection, Member of managed instance group and unmanaged instance group. Multiple filter conditions, then all must be true for a VM to be listed unless you explicitly state the OR operator
- To add a GPU to an instance, you must start an instance in which GPU libraries have been installed or will be installed. Also verify that the instance will run in a zone
that has GPUs available. And CPU must be compatible with the GPU selected, and GPUs cannot be attached to shared memory machines, and must set the instance to terminate during maintenance
- When first create a snapshot, GCP will make a full copy of the data on the persistent disk. The next time create a snapshot from that disk, GCP will copy only the data that has changed since the last snapshot. This optimizes storage while keeping the snapshot up to date with the data that was on the disk the last time a snapshot operation occurred. Snapshots are copies of disks and are useful as backups and for copying data to other instances
- It is a good practice to label all resources with a consistent labeling convention
- Images are used to create VMs, can be created from the following: Disk, Snapshot, Cloud storage file, Another image. Images have an optional attribute called Family, which allows you to group images. Eventually, deprecated images will no longer be available
- Command line: --flatten, --format, --verbosity, --async, --keep-disks=all, --delete-disks=data, --filter="zone:ZONE"
- Managed groups consist of groups of identical VMs. They are created using an instance template, which is a specification of a VM configuration, including machine type, boot disk image, zone, labels, and other properties of an instance. Managed instance groups can automatically scale the number of instances in a group and be used with load balancing to distribute workloads across the instance group. If an instance in a group crashes, it will be recreated automatically. Managed groups are the preferred type of instance group
- Unmanaged groups should be used only when you need to work with different configurations within different VMs within the group
- Instance groups are sets of instances managed as a single entity. Instance groups can contain instances in a single zone or across a region. The first is called a zonal managed instance group, and the second is called a regional managed instance group. Regional managed instance groups are recommended because that configuration spreads the workload across zones, increasing resiliency
- In addition to load balancing, managed instance groups can be configured to autoscale. You can configure an autoscaling policy to trigger adding or removing instances based on CPU utilization, monitoring metric, load-balancing capacity, or queue-based workloads
- Instances are created automatically when an instance group is created


Chapter 7 - Computing with Kubernetes
-------------------------------------

- Pods treat the multiple containers as a single entity for management purposes. Replicas are copies of pods and constitute a group of pods that are managed as a unit. Pods support autoscaling as well. Pods are considered ephemeral; that is, they are expected to terminate. Pods are single instances of a running process in a cluster. Pods run containers but are not sets of containers
- Service is an object that provides API endpoints with a stable IP address that allow applications to discover pods running a particular application. Services update when changes are made to pods, so they maintain an up-to-date list of pods running an application. Services provide a level of indirection to accessing pods
- ReplicaSet is a controller used by a deployment that ensures the correct number identical of pods are running
- Deployments are sets of identical pods. The members of the set may change as some pods are terminated and others are started, but they are all running the same application
- StatefulSets are like deployments, but they assign unique identifiers to pods. This enables Kubernetes to track which pod is used by which client and keep them together. StatefulSets are used when an application needs a unique network identifier or stable persistent storage
- Job is an abstraction about a workload. Jobs create pods and run them until the application completes a workload
- The first time you use Kubernetes Engine, you may need to create credentials
- Kubernetes creates instance groups as part of the process of creating a cluster. Multizone/multiregion clusters are available in Kubernetes Engine and are used to provide resiliency to an application
- **kubectl** commands specify a verb and then a resource. **kubectl** command is used to control workloads on a Kubernetes cluster once it is created, like run a Docker image on a cluster. **kubectl**, not gcloud, is used to initiate deployments
- Stackdriver is a comprehensive monitoring, logging, alerting, and notification service that can be used to monitor Kubernetes clusters
- Workspaces are logical structures for storing information about resources in a project that are being monitored
- Alerts are assigned to instances or sets of instances


Chapter 8 - Managing Kubernetes Clusters
----------------------------------------

- **gcloud ** command is used to view, modify Kubernetes resources such as clusters, nodes, Container Registry images, which managed by GCP
- **gcloud container clusters get-credentials** command is the correct command to configure kubectl to use GCP credentials for the cluster
- **gcloud container clusters create** ch07-cluster --num-nodes=3 --region=us-central1
- **gcloud container clusters resize** standard-cluster-1 --node-pool default-pool --size 5 --region=us-central1, command requires the name of the cluster and the node pool to modify
- **gcloud container clusters update** standard-cluster-1 **--enable-autoscaling --min-nodes 1 --max-nodes 5** --zone us-central1-a --node-pool default-pool, to enable autoscaling, use the update command to specify
the maximum and minimum number of nodes
- Pods are used to implement replicas of a deployment. Pods are managed through deployments. A deployment includes a configuration parameter called **replicas**, which are the number of pods running the application specified in the deployment. It is a best practice to modify the deployments, which are configured with a specification of the number of replicas that should always run
- Deployments are listed under Workloads in Kubernetes Engine menu
- In Create Deployment page in Cloud Console, can specify container image, cluster name, application name along with the labels, initial command, and namespace
- Actions in Deployment details are: **Autoscale**, **Expose**, **Rolling Update**, **Scale**
- **kubectl** command is used to view, modify Kubernetes resources such as pods, deployments, services, which managed by Kubernetes
- **kubectl run** hello-server --image=gcr.io/google/samples/hello-app:1.0 --port 8080, is the command used to start a deployment. It takes a name for the deployment, an image, and a port specification
- **kubectl expose deployment** hello-server --type="LoadBalancer", command makes a service accessible
- **kubectl get deployments** to list deployments
- **kubectl scale deployment** to modify the number of deployments
- **kubectl autoscale deployment** to enable autoscaling.
- **kubectl get services**, command to list services
- **kubectl delete service** hello-server
- **kubectl set image deployment** [DEPLOYMENT NAME] [IMAGE NAME], is the command to update the image of a deployment imperatively without editing the manifest template; it will perform a rolling update on existing deployment to use the new image
- The Container Registry is the service for managing images that can be used in other services, including Kubernetes Engine and Compute Engine
- **gcloud container images** list --repository gcr.io/google-containers
- **gcloud container images** describe gcr.io/appengflex-project-1/nginx
- In Kubernetes, IP addresses are assigned to VMs, not services


Chapter 9 - Computing with App Engine
-------------------------------------

- App Engine **Standard** and App Engine **Flexible**
- App Engine **Standard** applications consist of four components: Application -> Service -> Version -> Instance
- A project can support only one App Engine app. If you’d like to run other applications, they will need to be placed in their own projects
- All resources associated with an App Engine app are created in the region specified when the app is created
- Services are defined by their source code and their configuration file. The combination of those files constitutes a version of the app
- in **app.yaml** file **runtime** parameter specifies the language environment to execute in; **script** parameter specifies the script to execute; there is no parameter for specifying the maximum time an application can run
- **gcloud app deploy app.yaml** is used to deploy an App Engine app from the command line. It breaks **gcloud [service] [resource] verb** command line convention. This command must be executed from the directory with the **app.yaml** file. **--no-promote** parameter is to deploy the app without routing traffic to it. It is the way to get code out as soon as possible without exposing it to customers
- **gcloud app logs** command
- **gcloud app browse** command
- **gcloud app versions stop** command
- App Engine applications are accessible from URLs that consist of the project name followed by appspot.com. Can also assign a custom domain rather not **appspot.com** URL. Do this from the Add New Custom domain function on the App Engine Settings page
- Two kinds of instances available in App Engine Standard - **resident instances** are resident and running all the time, optimized for performance so users will wait less while an instance is started, used with **manual scaling**; **dynamic instances** are scaled based on load, used with **autoscaling and basic scaling**
- Autoscaling enables: **target_cpu_utilization**, **target_throughput_utilization**, **max_concurrent_requests**, **max_instances**, **min_instances**, **max_pending_latency**, **min_pending_latency**
- **target_cpu_utilization** specifies the maximum CPU utilization that occurs before additional instances are started
- **target_throughput_utilization** specifies the maximum number of concurrent requests before additional instances are started, uses a 0.05 to 0.95 scale to specify maximum throughput utilization
- **max_concurrent_requests** specifies the max concurrent requests an instance can accept before starting a new instance. The default is 10; the max is 80
- **max_instances** / **min_instances** specifie the maximum / minimum number of instances that can run for this application
- **max_pending_latency** / **min_pending_latency** indicates the maximum and minimum time a request will wait in the queue to be processed
- Basic scaling only allows parameters for **idle_timeout** and **max_instances**
- Manual scaling only allows parameter for **instances**
- **IP address**, **HTTP cookie** (preferred way), and **random splitting**, are allowed methods for splitting traffic
- The cookie used for splitting in App Engine is called **GOOGAPPUID**
- **gcloud app services set-traffic** command allocates service to some users to the new version without exposing all users to it. If no service name is specified, then all services are split; **set-traffic** command takes the following parameters: **--split** is the mandatory parameter for specifying a list of instances and the percent of traffic they should receive; **--migrate** migrate traffic from the previous version to the new version; **--split-by** values are ip, cookie, and random;


Chapter 10 - Computing with Cloud Functions
-------------------------------------------

- App Engine supports multiple services organized into a single application
- Cloud Functions supports individual services that are managed and operate independently of other services. Cloud Functions will time out after 1 minute, although you can set the timeout for as long as 9 minutes
- **Events** categories: Cloud Storage, Cloud Pub/Sub, HTTP, Firebase, Stackdriver Logging
- **Trigger** is a way of responding to an event
- **Triggers** have an associated **Function**
- **Function** takes two arguments: event_data and event_context
- **Function** memory options range from 128MB to 2GB, default is 256MB
- **Function** parameters for **Cloud Storage**: Cloud function name, Memory allocated for the function, Trigger, **Event type**, Source of the function code, Runtime, Source code, Name of the function to execute
- **Function** parameters for **Cloud Pub/Sub**: Cloud function name, Memory allocated for the function, Trigger, **Topic**, Source of the function code, Runtime, Source code, Name of the function to execute
- Parameters creating Cloud Storage function: runtime, trigger-resource, trigger-event. Trigger events are: google.storage.object.finalize, google.storage.object.delete, google.storage.object.archive, google.storage.object.metadataUpdate
- Parameters creating Cloud Pub/Sub function: runtime, trigger-topic. Trigger event is: topic


Chapter 11 - Planning Storage in the Cloud
------------------------------------------

- Memorystore can be configured to use between 1GB and 300GB of memory
- Persistent disks, both SSD and HDD can be up to 64TB. Persistent disks automatically encrypt data on the disk
- Four storage classes in **Cloud Storage**: Regional, multiregional, nearline, and coldline
- Cloud Storage uses an object data model
- Lifecycle rule can be  specified on objects in Cloud Storage. Condition options: Age, Creation Data, Storage Class, Newer Versions, and Live State (live or
archived versions of an object)
- Lifecycle on Cloud Storage: Regional and multiregional class can be changed to nearline or coldline; Nearline storage class can change to coldline. Regional class storage cannot be changed to multiregional. Multiregional class cannot be changed to regional
- When versioning is enabled on a bucket, a copy of an object is archived each time the object is overwritten or when it is deleted. The most recent version of an object on bucket is called the **Live version**
- There are three broad categories of data models available in GCP: object, relational, and NoSQL. Cloud Firestore and Firebase as a fourth category
- Cloud SQL and Cloud Spanner use relational databases for transaction processing applications; BigQuery uses a relational model for data warehouse and analytic applications
- The first task for using BigQuery is to create a data set to hold data, by clicking Create Dataset
- Datastore and Firebase are document databases
- Datastore has some features in common with relational databases, such as support for transactions and indexes to improve query performance. The main difference is that Datastore does not require a fixed schema or structure and does not support relational operations, such as joining tables, or computing aggregates, such as sums and counts.
- Cloud Firestore is that it is designed for storing, synchronizing, and querying data across distributed applications, like mobile apps. Apps can be automatically updated in close to real time when data is changed on the backend. Cloud Firestore supports transactions and provides multiregional replication.
- Bigtable is a wide-column table
- Data stores decision: Read and write patterns, consistency requirements, transaction support, cost, and latency ...
- Cloud SQL and Bigtable require you to specify some configuration information for VMs
- Second-generation instance, can configure the MySQL version, connectivity, machine type, automatic backups, failover replicas, database flags, maintenance windows, and labels


Chapter 12 - Deploying Storage in Google Cloud Platform
-------------------------------------------------------

- Query the document database using GQL, a query language similar to SQL
- **gcloud** is used for most products but not all; **gsutil** is used to work with Cloud Storage from the command line; **bq** used for BigQuery from the command line; **cbt** used to work with Bigtable from the command line
- gcloud sql backups create
- gcloud sql instances patch ace-exam-mysql --backup-start-time 03:00
- gcloud datastore export –namespaces='[NAMESPACE]' gs://ace_exam_backups
- gcloud datastore import gs://[BUCKET]/[PATH]/[FILE].overall_export_metadata
- BigQuery displays an estimate of the amount of data scanned. Use the scanned data estimate with the **Pricing Calculator** to get an estimate cost
- In BigQuery console Job History shows active jobs, completed jobs, and jobs that generated errors
- bq --location=[LOCATION] query --use_legacy_sql=false --dry_run [SQL_QUERY]
- bq --location=US show -j gcpace-project:US.bquijob_119adae7_167c373d5c3
- Subscriptions can be pulled, in which the application reads from a topic, or pushed, in which the subscription writes messages to an endpoint
- Pub/Sub will wait the period of time specified in the Acknowledgment Deadline parameter. The time to wait can range from 10 to 600 seconds
- gcloud pubsub topics create [TOPIC-NAME]
- gcloud pubsub topics publish [TOPIC_NAME] --message [MESSAGE]
- gcloud pubsub subscriptions create [SUBSCRIPTION-NAME] --topic [TOPIC-NAME]
- gcloud pubsub subscriptions pull --auto-ack [SUBSCRIPTION_NAME]
- Unread messages have a retention period after which they are deleted
- cbt createtable ace-exam-bt-table
- cbt ls
- cbt createfamily ace-exam-bt-table colfam1
- cbt set ace-exam-bt-table row1 colfam1:col1=ace-exam-value
- cbt read ace-exam-bt-table
- gcloud dataproc clusters create cluster-bc3d --zone us-west2-a
- gcloud dataproc jobs submit spark --cluster cluster-bc3d --jar ace_exam_jar.jar
- gsutil rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT]
- gsutil mv gs://[SOURCE_BUCKET_NAME]/[SOURCE_OBJECT_NAME] gs://[DESTINATION_BUCKET_NAME]/[DESTINATION_OBJECT_NAME]
- gsutil mv gs://[BUCKET_NAME]/[OLD_OBJECT_NAME] gs://[BUCKET_NAME]/[NEW_OBJECT_NAME]


Chapter 13 - Loading Data into Storage
--------------------------------------

- The first step in loading data into Cloud Storage is to create a bucket
- Folder can't be moved in GCP Console, under Storage menu
- gsutil mb gs://[BUCKET_NAME]/
- gsutil cp [LOCAL_OBJECT_LOCATION] gs://[DESTINATION_BUCKET_NAME]/
- gsutil mv gs://[SOURCE_BUCKET_NAME]/[SOURCE_OBJECT_NAME] gs://[DESTINATION_BUCKET_NAME]/[DESTINATION_OBJECT_NAME]
- gsutil acl ch -u [SERVICE_ACCOUNT_ADDRESS]:W gs://[BUCKET_NAME]
- gcloud sql instances describe [INSTANCE_NAME]
- gcloud sql export sql|csv [INSTANCE_NAME] gs://[BUCKET_NAME]/[EXPORT_FILE_NAME] --database=[DATABASE_NAME]
- gcloud sql import sql|csv [INSTANCE_NAME] gs://[BUCKET_NAME]/[IMPORT_FILE_NAME] --database=[DATABASE_NAME]
- Exports and imports of Cloud Datastore are done at the level of **namespaces**. The default namespace for Cloud Datastore is **default**
- Cloud Datastore export process creates a metadata file with information about the data exported and a folder that has the data itself. Export folder name is using the data and time of the export, e.g., **gcloud datastore export --namespaces="(default)" gs://ace-exam-bucket1**; when import, e.g., **gcloud datastore import gs://ace-exam-datastore1/2018-12-20T19:13:55_64324/2018-12-20T19:13:55_64324.overall_export_metadata**
- BigQuery export format options are CSV, Avro, and JSON. Choose a compression type. The options are None or Gzip for CSV and “**deflate**” and “**snappy**” for Avro
- **Avro** is a compact binary format that supports complex data structures, a schema is written to the file along with data. Schemas are defined in JSON. Avro is a good option for large data sets, and compressed using either the **deflate** or **snappy** utilities
- BigQuery imported, file format options include CSV, JSON, Avro, Parquet, PRC, and Cloud Datastore Backup
- BigQuery table type may be **native type** or **external table**. If the table is external, the data is kept in the source location, and only metadata about the table is stored in BigQuery. This is used when you have large data sets and do not want to load them all into BigQuery
- To export BigQuery data from the command line, use the **bq extract** command: bq extract --destination_format [FORMAT] --compression [COMPRESSION_TYPE] --field_delimiter [DELIMITER] --print_header [BOOLEAN] [PROJECT_ID]:[DATASET].[TABLE] gs://[BUCKET]/[FILENAME]
- To import data into BigQuery from the command line, use the **bq load** command: bq load --autodetect --source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_SOURCE]. **--autodetect** automatically detect the schema of a file on import
- Export from Cloud Spanner will be charges for running **Cloud Dataflow**, a pipeline service for processing streaming and batch data that implements workflows, because there is no gcloud command to export data, and there may be data egress charges for data sent between regions
- Cloud Bigtable does not have an Export and Import option in the Cloud Console or in gcloud. Two other options: using a Java application for importing and exporting or using the HBase interface to execute HBase commands
- Cloud Dataproc is a data analysis platform. These platforms are designed more for data manipulation, statistical analysis, machine learning, and other complex operations than for data storage and retrieval. When you export from Dataproc, you are exporting the cluster configuration, not data in the cluster
- gcloud beta dataproc clusters export [CLUSTER_NAME] --destination=[PATH_TO_EXPORT_FILE]
- gcloud beta dataproc clusters import [SOURCE_FILE]


Chapter 14 - Creating a Virtual Private Cloud with Subnets
----------------------------------------------------------

- GCP automatically creates a VPC when you create a project
- VPCs are global resources, so they are not tied to a specific region or zone
- VPCs are logical data centers in the cloud. VPCs are global, they have subnets in all regions. Resources in any region can be accessed through the VPC, can communicate with each other in SAME VPC
- VPCs can have multiple subnets but each subnet has its own address range, subnets are regional resources
- The shared VPC is hosted in a common project. Users in other projects who have sufficient permissions can create resources in the shared VPC
- Classless Inter Domain Routing (CIDR) notation
- Private Google Access allows VMs on the subnet to access Google services without assigning an external IP address to the VM
- Flow Logs option turns on / off logging of network traffic and sent to Stackdriver
- Regional routing will have Google Cloud Routers learn routes within the region. Global routing will enable Google Cloud Routers to learn routes on all subnetworks in the VPC
- gcloud compute networks create ace-exam-vpc1 --subnet-mode=auto (**auto mode network** is GCP chooses a range of IP addresses for each subnet when creating subnets)
- gcloud compute networks create ace-exam-vpc1 --subnet-mode=custom
- gcloud beta compute networks **subnets** create ace-exam-vpc-subnet1 --network=aceexam-vpc1 --region=us-west2 --range=10.10.0.0/16 --enable-private-ip-googleaccess --enable-flow-logs
- gcloud organizations add-iam-policy-binding [ORG_ID] --member='user:[EMAIL_ADDRESS]' --role="roles/compute.xpnAdmin" (Shared VPC Admin role to a organisation)
- gcloud organizations list
- gcloud beta resource-manager **folders** add-iam-policy-binding [FOLDER_ID] --member='user:[EMAIL_ADDRESS]' --role="roles/compute.xpnAdmin" (Shared VPC Admin role to a folder)
- gcloud beta resource-manager **folders** list --organization=[ORG_ID]
- Shared VPCs can be shared at the **network or folder level**. Shared VPCs need to bind identity and access management (IAM) policies at the **organizational or folder level** to enable Shared VPC Admin roles
- gcloud compute shared-vpc enable [HOST_PROJECT_ID] (sharing VPC at the organisation level)
- gcloud compute shared-vpc associated-projects add [SERVICE_PROJECT_ID] --host-project [HOST_PROJECT_ID] (sharing VPC at the organisation level)
- gcloud beta compute shared-vpc enable [HOST_PROJECT_ID] (sharing VPC at the folder level)
- gcloud beta compute shared-vpc associated-projects add [SERVICE_PROJECT_ID] --host-project [HOST_PROJECT_ID] (sharing VPC at the folder level)
- **VPC peering** for interproject connectivity
- gcloud compute networks peerings create peer-ace-exam-1 --network ace-exam-network-A --peer-project ace-exam-project-B --peer-network ace-exam-network-B --auto-create-routes (peering on network from A to B)
- gcloud compute networks peerings create peer-ace-exam-1 --network ace-exam-network-B --peer-project ace-exam-project-A --peer-network ace-exam-network-A --auto-create-routes (peering on network from B to A)

- Firewall is stateful which means if traffic is allowed in one direction and a connection established, it is allowed in the other direction
- An active connection is one with at least one packet exchanged every ten minutes
- All VPCs start with two **implied rules**: One allows egress traffic to all destinations (IP address 0.0.0.0/0), and one denies all incoming traffic from any source (IP address 0.0.0.0/0). **implied rules** can't be deleted
- Firewall rules consist of direction (incoming / outcoming), priority (which of all the matching rules is applied), action (allow / deny), target (an instance, alll instances in a network, instances with particular network tags, instances using a special service account), source (IP ranges, instances with particular network tags, instances using a special service account) / destination (IP ranges), protocols (TCP, UDP, ICMP) and port, and enforcement status (enabled / disabled)
- Compute and the resource used for creating, deleting, describing, updating, listing a firewall rule
- Firewall rules are only applied to subnet level, can't to VPC level
- gcloud compute firewall-rules create ace-exam-fwr1 –-network ace-exam-vpc1 –-allow tcp:20000-25000
- gcloud compute firewall-rules create ace-exam-fwr1 –-direction ingress –-allow udp:20000-30000

- VPNs are secure connections between your VPC subnets and your internal network. VPNs route traffic between your cloud resources and your internal network. VPNs include gateways, forwarding rules, and tunnels (**gcloud compute forwarding-rule**, **gcloud compute target-vpn-gateways**, **gcloud compute vpn-tunnels**)
- Routers can be configured to learn **regional routes** or **global routes**. Global dynamic routing is used to learn all routes on a network. Regional dynamic routing would learn only routes in a region
- Dynamic (routes are learned regionally or globally), Route-Based (IP ranges of the remote network), or Policy-Based Routing (remote IP ranges, local subnet, local IP ranges)
- Dynamic routing uses the Board Gateway Protocol (**BGP protocol**) to learn routes in your networks. Private **Autonomous System Number (ASN)** used by the BGP protocol. The ASN is a number in the range 64512–65534 or 4000000000–4294967294. Each cloud router you create will need a **unique ASN**
- Internet Key Exchange (IKE) protocol
- gcloud compute target-vpn-gateways create NAME --network [VPN_NETWORK] --region [REGION]
- gcloud compute forwarding-rules create NAME --TARGET_SPECIFICATION (--target-instance, --target-http-proxy, --target-vpn-gateway) [VPN_GATEWAY]
- gcloud compute vpn-tunnels create NAME --peer-address [PEER_ADDRESS] (IPv4 address of the remote tunnel endpoint) --sharedsecret [SHARED_SECRET] --target-vpn-gateway [TARGET_VPN_GATEWAY] (target VPN gateway IP)


Chapter 15 - Networking in the Cloud: DNS, Load Balancing, and IP Addressing
----------------------------------------------------------------------------

- Domain Name System (DNS)
- HTTP(S), SSL Proxy, TCP Proxy, Network TCP/UDP, and Internal TCP/UDP Network
- A record maps a hostname to IP addresses in IPv4
- AAAA records are used in IPv6 to map names to IPv6 addresses
- CNAME records hold the canonical name. CNAME record takes a name, or alias of a server. The DNS name and TTL parameters are the same as in the A record
- DNSSEC (DNS security) is designed to prevent spoofing (a client appearing to be some other client) and cache poisoning (a client sending incorrect information to update the DNS server)
- NS (Name Server)
- SOA (Start Of Authority)
- TTL (Time To Live)
- DNS Forwarding allows your DNS queries to be passed to an on-premise DNS server if you are using Cloud VPN or Interconnect
- gcloud beta dns managed-zones create ace-exam-zone1 --description= --dnsname=aceexamzone.com.
- gcloud beta dns managed-zones create ace-exam-zone1 --description= --dnsname=aceexamzone.com. --visibility=private --networks=default
- To add an A record, start a transaction, add the A record information, execute the transaction:
** gcloud dns record-sets transaction **start** --zone=ace-exam-zone1
** gcloud dns record-sets transaction **add** 192.0.2.91 --name=aceexamzone.com. --ttl=300 **--type=A** --zone=ace-exam-zone1
** gcloud dns record-sets transaction **execute** --zone=ace-exam-zone1.
- To create a CNAME record:
** gcloud dns record-sets transaction **start** --zone=ace-exam-zone1
** gcloud dns record-sets transaction **add** server1.aceexamezone.com. --name=www2.aceexamzone.com. --ttl=300 **--type=CNAME** --zone=ace-exam-zone1
** gcloud dns record-sets transaction **execute** --zone=ace-exam-zone1
- Reserved addresses stay attached to a VM when it is not in use and stay attached until released
- Ephemeral addresses are released automatically when a VM shuts down
- gcloud beta compute addresses create ace-exam-reserved-static1 --region=us-west2 --network-tier=PREMIUM

- Global Load Balancers
** HTTP(S)
** SSL Proxy, non-HTTPS traffic
** TCP Proxy, configure both the frontend (specify ports to forward when configuring the frontend) and backend (backend is where you configure how traffic is routed to VMs)
- Regional Load Balancers:
** Internal TCP/UDP
** Network TCP/UDP, based on IP protocol, address, and port, are used for SSL and TCP traffic not supported by the SSL Proxy and TCP Proxy load balancers
- External Load Balancer:
** HTTP(S)
** SSL Proxy
** TCP Proxy
** Network TCP/UDP
- Internal Load Balancer, balance traffic only from within GCP:
** Internal TCP/UDP
- The prefix length specifies the length in bits of the subnet mask
- gcloud compute target-pools add-instances ace-exam-pool --instances ig1,ig2
- gcloud compute forwarding-rules create ace-exam-lb --port=80 --target-pool ace-exam-pool
- gcloud compute networks subnets expand-ip-range ace-exam-subnet1 --prefix-length 16


Chapter 16 - Deploying Applications with Cloud Launcher and Deployment Manager
------------------------------------------------------------------------------

- Cloud Launcher / Marketplace categories are: Datasets, Operationg System, Developer Tools, Kubernetes Apps, API & Services, Databases
- Deployment Manager is a GCP service for creating configuration files that define resources to use with an application
- Deployment Manager addresses that problem by making it relatively simple to deploy an application and resources in a repeatable process
- Deployment Manager configuration files can be long or complicated, you can modularize them using templates. Templates define resources and can be imported into other templates. Template can be written in Python or Jinja2. Google recommends using Python for complicated templates
- Free, Paid, and Bring You Own Licence (BYOL) are all license options used in Cloud Launcher
- Deployment Manager configuration files start with the word **resources**, followed by resource entities, which are defined using three fields:
** **name**, which is the name of the resource
** **type**, which is the type of the resource, such as compute.v1.instance
** **properties**, which are key-value pairs that specify configuration parameters for the resource. For example, a VM has properties to specify machine type, disks, and network interfaces
- gcloud **deployment-manager** deployments create quickstart-deployment --config vm.yaml
- gcloud **deployment-manager** deployments describe quickstart-deployment
- gcloud **deployment-manager** deployments list
- gcloud compute list images


Chapter 17 - Configuring Access and Security
--------------------------------------------

- **Least privileges**, **Separation Of Duties** (ensures that two or more people are required to complete a sensitive task), **Defense In Depth** (combines multiple security controls)
- Access controls in GCP are managed using **Primitive Roles** (provide coarse-grained access controls to resources), **IAM** (Identity and Access Management, predefined roles are collections of permissions), **Scopes** (access control, permissions granted to a **VM** to perform some operation, used to limit operations that can be performed by an instance, specified using a URL that starts with https://www.googleapis.com/auth/ and is then followed by permission on a resource)
- Use IAM roles to constrain scopes and use scopes to constrain IAM roles
- Primitive Roles:
** **Owner**: Owners have editor permissions and can manage roles and permission on an entity, can also set up billing for a project
** **Editor**: Editors have viewer permissions and permission to modify an entity
** **Viewer**: Viewers have permission to perform read-only operations
- Custome Role launch stage: **Alpha**, **Beta**, **General Availability**, **Disabled**. Not all permissions are available for use in a Custom Role
- **Service Accounts** are used to provide identities independent of human users. **Service Accounts** are identities that can be granted roles
- **Scopes** are permissions granted to a VM to perform some operation. **Scopes** are access controls that apply to instances of VMs. **Scopes** authorize the access to API methods
- **Service account** assigned to a VM has roles associated with it. To configure access controls for a VM, you will need to configure both IAM roles and scopes
- A VM instance can only perform operations allowed by both IAM roles assigned to the service account and scopes defined on the instance
- **Accessing scopes** options when creating an instance: **Default Access**, **Full Access to all Cloud APIs**, **Set Access for Each API**
- View Audit Logs in Stackdriver by **resource**, **types of logs to display**, **log level**, **period of time**

- gcloud projects get-iam-policy ace-exam-project
- gcloud projects add-iam-policy-binding ace-exam-project --member user:jane@aceexam.com --role roles/appengine.deployer
- gcloud iam roles describe roles/appengine.deployer
- gcloud iam roles create customAppEngine1 --project ace-exam-project --title='Custom Update App Engine' --description='Custom update' --permissions=appengine.applications.update --stage=alpha
- gcloud compute instances **set-service-account** ace-instance **--service-account** examadmin@ace-exam-project.iam.gserviceaccount.com --scopes compute-rw,storage-ro
- gcloud compute instances create [INSTANCE_NAME] **--service-account** [SERVICE_ACCOUNT_EMAIL]


Chapter 18 - Monitoring, Logging, and Cost Estimating
-----------------------------------------------------

- **Aligning** is the process of grouping data that arrives within a time into regular buckets of time, functions including min, max, mean, count, sum ...
- **Aggregation** specifies a reducer, which is a function for combining values in a group of time series to produce a single value. It is used to combine data points using common statistics, such as sum, min, max, count ...
- OpenCensus provides a higher-level, monitoring-focused API, while the Stackdriver Monitoring API is lower-level
- **Gauges** are measures at a point in time, **Deltas** capture the change over an interval, **Cumulative** are accumulated values over an interval
- Logging to a storage system is called **exporting**, the location to which you write the log data is called a **sink**
- Label or Text Search, Resource Type, Log Type, Time Limit, Log Level can be used to filter log entries when viewing logs in Stackdriver Logging
- Log Level statuses include Critical, Error, Warning, Info, Debug
- Cloud Trace is a distributed tracing application that helps developers and DevOps engineers identify sections of code that are performance bottlenecks
- Cloud Debug provides for creating snapshots of running code and injecting log messages without altering source code
- Logpoint, which is a log statement that is written to the log when the statement executes


References
----------

- Official Google Cloud Certified Associate Cloud Engineer Study Guide, _https://www.wiley.com/en-au/Official+Google+Cloud+Certified+Associate+Cloud+Engineer+Study+Guide-p-9781119564393_
- QwikLabs Free Codes — GCP and AWS, _https://medium.com/@sathishvj/qwiklabs-free-codes-gcp-and-aws-e40f3855ffdb_
- Get GCP Certified - GCP Fundamentals, _https://www.getgcpcertified.com/courses/613442/lectures/10982362_
- Get GCP Certified - GCP Compute & Storage Solutions, _https://www.getgcpcertified.com/courses/628044/lectures/11207103_
- Get GCP Certified - Associate Cloud Engineer, _https://www.getgcpcertified.com/p/free-trial-associate-cloud-engineer_